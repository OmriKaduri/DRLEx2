{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 1 Reward: 21.0 Average over 100 episodes: 0.0\n",
      "Episode 2 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 3 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 4 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 5 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 6 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 7 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 8 Reward: 26.0 Average over 100 episodes: 0.0\n",
      "Episode 9 Reward: 27.0 Average over 100 episodes: 0.0\n",
      "Episode 10 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 11 Reward: 24.0 Average over 100 episodes: 0.0\n",
      "Episode 12 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 13 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 14 Reward: 9.0 Average over 100 episodes: 0.0\n",
      "Episode 15 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 16 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 17 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 18 Reward: 20.0 Average over 100 episodes: 0.0\n",
      "Episode 19 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 20 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 21 Reward: 40.0 Average over 100 episodes: 0.0\n",
      "Episode 22 Reward: 17.0 Average over 100 episodes: 0.0\n",
      "Episode 23 Reward: 22.0 Average over 100 episodes: 0.0\n",
      "Episode 24 Reward: 27.0 Average over 100 episodes: 0.0\n",
      "Episode 25 Reward: 30.0 Average over 100 episodes: 0.0\n",
      "Episode 26 Reward: 35.0 Average over 100 episodes: 0.0\n",
      "Episode 27 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 28 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 29 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 30 Reward: 18.0 Average over 100 episodes: 0.0\n",
      "Episode 31 Reward: 9.0 Average over 100 episodes: 0.0\n",
      "Episode 32 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 33 Reward: 8.0 Average over 100 episodes: 0.0\n",
      "Episode 34 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 35 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 36 Reward: 9.0 Average over 100 episodes: 0.0\n",
      "Episode 37 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 38 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 39 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 40 Reward: 26.0 Average over 100 episodes: 0.0\n",
      "Episode 41 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 42 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 43 Reward: 25.0 Average over 100 episodes: 0.0\n",
      "Episode 44 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 45 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 46 Reward: 12.0 Average over 100 episodes: 0.0\n",
      "Episode 47 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 48 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 49 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 50 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 51 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 52 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 53 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 54 Reward: 22.0 Average over 100 episodes: 0.0\n",
      "Episode 55 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 56 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 57 Reward: 28.0 Average over 100 episodes: 0.0\n",
      "Episode 58 Reward: 24.0 Average over 100 episodes: 0.0\n",
      "Episode 59 Reward: 27.0 Average over 100 episodes: 0.0\n",
      "Episode 60 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 61 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 62 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 63 Reward: 12.0 Average over 100 episodes: 0.0\n",
      "Episode 64 Reward: 22.0 Average over 100 episodes: 0.0\n",
      "Episode 65 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 66 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 67 Reward: 17.0 Average over 100 episodes: 0.0\n",
      "Episode 68 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 69 Reward: 12.0 Average over 100 episodes: 0.0\n",
      "Episode 70 Reward: 30.0 Average over 100 episodes: 0.0\n",
      "Episode 71 Reward: 26.0 Average over 100 episodes: 0.0\n",
      "Episode 72 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 73 Reward: 12.0 Average over 100 episodes: 0.0\n",
      "Episode 74 Reward: 12.0 Average over 100 episodes: 0.0\n",
      "Episode 75 Reward: 12.0 Average over 100 episodes: 0.0\n",
      "Episode 76 Reward: 33.0 Average over 100 episodes: 0.0\n",
      "Episode 77 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 78 Reward: 18.0 Average over 100 episodes: 0.0\n",
      "Episode 79 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 80 Reward: 26.0 Average over 100 episodes: 0.0\n",
      "Episode 81 Reward: 19.0 Average over 100 episodes: 0.0\n",
      "Episode 82 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 83 Reward: 31.0 Average over 100 episodes: 0.0\n",
      "Episode 84 Reward: 24.0 Average over 100 episodes: 0.0\n",
      "Episode 85 Reward: 11.0 Average over 100 episodes: 0.0\n",
      "Episode 86 Reward: 15.0 Average over 100 episodes: 0.0\n",
      "Episode 87 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 88 Reward: 12.0 Average over 100 episodes: 0.0\n",
      "Episode 89 Reward: 13.0 Average over 100 episodes: 0.0\n",
      "Episode 90 Reward: 9.0 Average over 100 episodes: 0.0\n",
      "Episode 91 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 92 Reward: 14.0 Average over 100 episodes: 0.0\n",
      "Episode 93 Reward: 32.0 Average over 100 episodes: 0.0\n",
      "Episode 94 Reward: 25.0 Average over 100 episodes: 0.0\n",
      "Episode 95 Reward: 24.0 Average over 100 episodes: 0.0\n",
      "Episode 96 Reward: 29.0 Average over 100 episodes: 0.0\n",
      "Episode 97 Reward: 10.0 Average over 100 episodes: 0.0\n",
      "Episode 98 Reward: 16.0 Average over 100 episodes: 0.0\n",
      "Episode 99 Reward: 20.0 Average over 100 episodes: 17.53\n",
      "Episode 100 Reward: 28.0 Average over 100 episodes: 17.67\n",
      "Episode 101 Reward: 12.0 Average over 100 episodes: 17.58\n",
      "Episode 102 Reward: 10.0 Average over 100 episodes: 17.54\n",
      "Episode 103 Reward: 18.0 Average over 100 episodes: 17.57\n",
      "Episode 104 Reward: 16.0 Average over 100 episodes: 17.6\n",
      "Episode 105 Reward: 12.0 Average over 100 episodes: 17.61\n",
      "Episode 106 Reward: 20.0 Average over 100 episodes: 17.7\n",
      "Episode 107 Reward: 24.0 Average over 100 episodes: 17.83\n",
      "Episode 108 Reward: 20.0 Average over 100 episodes: 17.77\n",
      "Episode 109 Reward: 20.0 Average over 100 episodes: 17.7\n",
      "Episode 110 Reward: 36.0 Average over 100 episodes: 17.86\n",
      "Episode 111 Reward: 15.0 Average over 100 episodes: 17.77\n",
      "Episode 112 Reward: 24.0 Average over 100 episodes: 17.91\n",
      "Episode 113 Reward: 11.0 Average over 100 episodes: 17.82\n",
      "Episode 114 Reward: 10.0 Average over 100 episodes: 17.83\n",
      "Episode 115 Reward: 69.0 Average over 100 episodes: 18.32\n",
      "Episode 116 Reward: 22.0 Average over 100 episodes: 18.43\n",
      "Episode 117 Reward: 28.0 Average over 100 episodes: 18.52\n",
      "Episode 118 Reward: 17.0 Average over 100 episodes: 18.49\n",
      "Episode 119 Reward: 25.0 Average over 100 episodes: 18.55\n",
      "Episode 120 Reward: 18.0 Average over 100 episodes: 18.59\n",
      "Episode 121 Reward: 13.0 Average over 100 episodes: 18.32\n",
      "Episode 122 Reward: 26.0 Average over 100 episodes: 18.41\n",
      "Episode 123 Reward: 28.0 Average over 100 episodes: 18.47\n",
      "Episode 124 Reward: 10.0 Average over 100 episodes: 18.3\n",
      "Episode 125 Reward: 13.0 Average over 100 episodes: 18.13\n",
      "Episode 126 Reward: 12.0 Average over 100 episodes: 17.9\n",
      "Episode 127 Reward: 24.0 Average over 100 episodes: 18.01\n",
      "Episode 128 Reward: 12.0 Average over 100 episodes: 18.03\n",
      "Episode 129 Reward: 12.0 Average over 100 episodes: 17.99\n",
      "Episode 130 Reward: 12.0 Average over 100 episodes: 17.93\n",
      "Episode 131 Reward: 16.0 Average over 100 episodes: 18.0\n",
      "Episode 132 Reward: 21.0 Average over 100 episodes: 18.02\n",
      "Episode 133 Reward: 24.0 Average over 100 episodes: 18.18\n",
      "Episode 134 Reward: 21.0 Average over 100 episodes: 18.23\n",
      "Episode 135 Reward: 26.0 Average over 100 episodes: 18.35\n",
      "Episode 136 Reward: 14.0 Average over 100 episodes: 18.4\n",
      "Episode 137 Reward: 25.0 Average over 100 episodes: 18.46\n",
      "Episode 138 Reward: 29.0 Average over 100 episodes: 18.6\n",
      "Episode 139 Reward: 27.0 Average over 100 episodes: 18.77\n",
      "Episode 140 Reward: 32.0 Average over 100 episodes: 18.83\n",
      "Episode 141 Reward: 12.0 Average over 100 episodes: 18.85\n",
      "Episode 142 Reward: 11.0 Average over 100 episodes: 18.82\n",
      "Episode 143 Reward: 38.0 Average over 100 episodes: 18.95\n",
      "Episode 144 Reward: 35.0 Average over 100 episodes: 19.19\n",
      "Episode 145 Reward: 19.0 Average over 100 episodes: 19.22\n",
      "Episode 146 Reward: 18.0 Average over 100 episodes: 19.28\n",
      "Episode 147 Reward: 13.0 Average over 100 episodes: 19.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 148 Reward: 24.0 Average over 100 episodes: 19.3\n",
      "Episode 149 Reward: 17.0 Average over 100 episodes: 19.28\n",
      "Episode 150 Reward: 18.0 Average over 100 episodes: 19.33\n",
      "Episode 151 Reward: 26.0 Average over 100 episodes: 19.4\n",
      "Episode 152 Reward: 14.0 Average over 100 episodes: 19.44\n",
      "Episode 153 Reward: 19.0 Average over 100 episodes: 19.49\n",
      "Episode 154 Reward: 16.0 Average over 100 episodes: 19.43\n",
      "Episode 155 Reward: 30.0 Average over 100 episodes: 19.57\n",
      "Episode 156 Reward: 87.0 Average over 100 episodes: 20.31\n",
      "Episode 157 Reward: 15.0 Average over 100 episodes: 20.18\n",
      "Episode 158 Reward: 30.0 Average over 100 episodes: 20.24\n",
      "Episode 159 Reward: 37.0 Average over 100 episodes: 20.34\n",
      "Episode 160 Reward: 25.0 Average over 100 episodes: 20.46\n",
      "Episode 161 Reward: 22.0 Average over 100 episodes: 20.53\n",
      "Episode 162 Reward: 16.0 Average over 100 episodes: 20.5\n",
      "Episode 163 Reward: 28.0 Average over 100 episodes: 20.66\n",
      "Episode 164 Reward: 23.0 Average over 100 episodes: 20.67\n",
      "Episode 165 Reward: 17.0 Average over 100 episodes: 20.65\n",
      "Episode 166 Reward: 17.0 Average over 100 episodes: 20.66\n",
      "Episode 167 Reward: 21.0 Average over 100 episodes: 20.7\n",
      "Episode 168 Reward: 11.0 Average over 100 episodes: 20.7\n",
      "Episode 169 Reward: 27.0 Average over 100 episodes: 20.85\n",
      "Episode 170 Reward: 32.0 Average over 100 episodes: 20.87\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-cb114f3c87cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mactions_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions_distribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions_distribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions_distribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omri\\projects\\study\\2020sema\\reinforcement\\reinforce-env\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omri\\projects\\study\\2020sema\\reinforcement\\reinforce-env\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omri\\projects\\study\\2020sema\\reinforcement\\reinforce-env\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omri\\projects\\study\\2020sema\\reinforcement\\reinforce-env\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omri\\projects\\study\\2020sema\\reinforcement\\reinforce-env\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\omri\\projects\\study\\2020sema\\reinforcement\\reinforce-env\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import datetime as dt\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [1, state_size], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "#             state_one_hot = tf.one_hot(, int(state_size))\n",
    "#             print(state_one_hot)\n",
    "            self.input_layer = tf.layers.dense(\n",
    "                inputs=self.state,\n",
    "                units=32,\n",
    "                activation='relu',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "            self.hidden_layer = tf.layers.dense(\n",
    "                inputs = self.input_layer,\n",
    "                units=16,\n",
    "                activation='relu',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "            self.output_layer = tf.layers.dense(\n",
    "                inputs=self.hidden_layer,\n",
    "                units=1,\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "#             self.loss = tf.reduce_sum(tf.square(self.target - self.value_estimate))\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "#             self.loss = tf.losses.mean_squared_error(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "    def predict(self, state, sess=None):\n",
    "#         state = state.reshape(-1)\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.value_estimate, {self.state: state})\n",
    "\n",
    "    def update(self, state, target, learning_rate, sess=None):\n",
    "#         state = state.reshape(-1)\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = {self.state: state, self.target: target, self.learning_rate:learning_rate}\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n",
    "\n",
    "class PolicyNetwork:\n",
    "    def __init__(self, state_size, action_size, name='policy_network'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "            self.state = tf.placeholder(tf.float32, [None, self.state_size], name=\"state\")\n",
    "            self.action = tf.placeholder(tf.int32, [self.action_size], name=\"action\")\n",
    "            self.R_t = tf.placeholder(tf.float32, name=\"total_rewards\")\n",
    "\n",
    "            self.W1 = tf.get_variable(\"W1\", [self.state_size, 12],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "            self.b1 = tf.get_variable(\"b1\", [12], initializer=tf.zeros_initializer())\n",
    "            self.W2 = tf.get_variable(\"W2\", [12, self.action_size],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "            self.b2 = tf.get_variable(\"b2\", [self.action_size], initializer=tf.zeros_initializer())\n",
    "\n",
    "            self.Z1 = tf.add(tf.matmul(self.state, self.W1), self.b1)\n",
    "            self.A1 = tf.nn.relu(self.Z1)\n",
    "            self.output = tf.add(tf.matmul(self.A1, self.W2), self.b2)\n",
    "\n",
    "            # Softmax probability distribution over actions\n",
    "            self.actions_distribution = tf.squeeze(tf.nn.softmax(self.output))\n",
    "            # Loss with negative log probability\n",
    "            self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.action)\n",
    "            self.loss = tf.reduce_mean(self.neg_log_prob * self.R_t)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "    \n",
    "# Define hyperparameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "\n",
    "max_episodes = 5000\n",
    "max_steps = 501\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.0001\n",
    "value_learning_rate = 0.005\n",
    "render = False\n",
    "\n",
    "# Initialize the policy network\n",
    "tf.reset_default_graph()\n",
    "policy = PolicyNetwork(state_size, action_size)\n",
    "value_estimator = ValueEstimator(state_size)\n",
    "\n",
    "LOGDIR = './TensorBoard/Q2' + f\"/DQLearning_{dt.datetime.now().strftime('%d%m%Y%H%M')}\"\n",
    "# Start training the agent with REINFORCE algorithm\n",
    "with tf.Session() as sess, tf.summary.FileWriter(LOGDIR) as tb_logger:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    solved = False\n",
    "    episode_rewards = np.zeros(max_episodes)\n",
    "    average_rewards = 0.0\n",
    "    step_done = 0\n",
    "\n",
    "    sliding_avg = collections.deque(maxlen=100)\n",
    "    avg_loss = 0.0\n",
    "    for episode in range(max_episodes):\n",
    "        if episode % 300 == 0 and episode > 0:\n",
    "            learning_rate*=0.1\n",
    "            value_learning_rate*=0.1\n",
    "            print(learning_rate, value_learning_rate)\n",
    "            \n",
    "        state = env.reset()\n",
    "        state = state.reshape([1, state_size])\n",
    "        episode_transitions = []\n",
    "        I = 1\n",
    "        for step in range(max_steps):\n",
    "\n",
    "            actions_distribution = sess.run(policy.actions_distribution, {policy.state: state})\n",
    "            action = np.random.choice(np.arange(len(actions_distribution)), p=actions_distribution)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            next_state = next_state.reshape([1, state_size])\n",
    "            episode_rewards[episode] += reward\n",
    "            reward = +1 if not done or step == 500 else -10\n",
    "            if render:\n",
    "                env.render()\n",
    "            value_s = value_estimator.predict(state)\n",
    "            value_next = 0\n",
    "            if done:\n",
    "                delta = reward - value_s\n",
    "            else:\n",
    "                value_next = value_estimator.predict(next_state)\n",
    "                delta = reward + (discount_factor*value_next) - value_s\n",
    "            \n",
    "            total_discounted_return = I*delta\n",
    "            value_estimator.update(state, reward + discount_factor*value_next, value_learning_rate)\n",
    "            \n",
    "            action_one_hot = np.zeros(action_size)\n",
    "            action_one_hot[action] = 1\n",
    "        \n",
    "            feed_dict = {policy.state: state, policy.R_t: total_discounted_return,\n",
    "                         policy.action: action_one_hot, policy.learning_rate:learning_rate}\n",
    "            _, loss = sess.run([policy.optimizer, policy.loss], feed_dict)\n",
    "#             if loss < -150:\n",
    "#                 print(step, total_discounted_return, reward, value_s, value_next)\n",
    "            avg_loss += loss\n",
    "\n",
    "            if done:\n",
    "                step_done = step + 1\n",
    "                sliding_avg.append(step_done)\n",
    "                if episode > 98:\n",
    "                    # Check if solved\n",
    "                    average_rewards = np.mean(episode_rewards[(episode - 99):episode + 1])\n",
    "                print(\"Episode {} Reward: {} Average over 100 episodes: {}\".format(episode, episode_rewards[episode],\n",
    "                                                                                   round(average_rewards, 2)))\n",
    "                if average_rewards > 475:\n",
    "                    print(' Solved at episode: ' + str(episode))\n",
    "                    solved = True\n",
    "                break\n",
    "                \n",
    "#             I = discount_factor*I\n",
    "            state = next_state\n",
    "\n",
    "        if solved:\n",
    "            break\n",
    "            \n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag='reward',\n",
    "                                                     simple_value=step_done),\n",
    "                                   tf.Summary.Value(tag='avg_loss',\n",
    "                                                     simple_value=avg_loss / step_done),\n",
    "                                   tf.Summary.Value(tag='reward_avg_100_eps',\n",
    "                                                     simple_value=sum(sliding_avg) / len(sliding_avg))])\n",
    "        tb_logger.add_summary(summary, episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
